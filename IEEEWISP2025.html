<!DOCTYPE html>
<html lang="en"><head>
<script src="IEEEWISP2025_files/libs/clipboard/clipboard.min.js"></script>
<script src="IEEEWISP2025_files/libs/quarto-html/tabby.min.js"></script>
<script src="IEEEWISP2025_files/libs/quarto-html/popper.min.js"></script>
<script src="IEEEWISP2025_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="IEEEWISP2025_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="IEEEWISP2025_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="IEEEWISP2025_files/libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="IEEEWISP2025_files/libs/quarto-contrib/videojs/video.min.js"></script>
<link href="IEEEWISP2025_files/libs/quarto-contrib/videojs/video-js.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.39">

  <meta name="author" content="Rozenn Dahyot">
  <meta name="dcterms.date" content="2025-04-08">
  <title>Geotagging of Objects</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="IEEEWISP2025_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="IEEEWISP2025_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="IEEEWISP2025_files/libs/revealjs/dist/theme/quarto-bbe7401fe57d4b791b917637bb662036.css">
  <link rel="stylesheet" href="styles.css">
  <link href="IEEEWISP2025_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="IEEEWISP2025_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="IEEEWISP2025_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="IEEEWISP2025_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="IEEEWISP2025_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="IEEEWISP2025_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script src="IEEEWISP2025_files/libs/quarto-diagram/mermaid.min.js"></script>
  <script src="IEEEWISP2025_files/libs/quarto-diagram/mermaid-init.js"></script>
  <link href="IEEEWISP2025_files/libs/quarto-diagram/mermaid.css" rel="stylesheet">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="./images/IEEEWISP2025/2025-04-08-WISP.png" data-background-opacity="0.5" data-background-size="contain" class="quarto-title-block center">
  <h1 class="title">Geotagging of Objects</h1>
  <p class="subtitle">Neural Networks based Solutions for Geotagging of Objects</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Rozenn Dahyot 
</div>
        <p class="quarto-title-affiliation">
            <a href="https://landing.signalprocessingsociety.org/apr-08-2025" target="_blank">IEEE WISP webinar</a>
          </p>
    </div>
</div>

  <p class="date">2025-04-08</p>
</section>
<section id="introduction" class="slide level2 smaller">
<h2>Introduction</h2>
<div class="cell" data-reveal="true" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<pre class="mermaid mermaid-js">timeline
    title  Some (online Data) Context
    2001: Google Earth
        : satellite &amp; aerial
    2004: OpenStreetMap
        : GIS
        : crowd sourcing
    2005: Google Map
        : GIS
    2006: Twitter
        : Geolocated multimedia posts
    2007: Google Street View
        : Road Scenes
    2013: Mapillary  
        : Road scenes
        : crowd sourcing
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<aside class="notes">
<p>Data availability, images and GIS:</p>
<ul>
<li>Google Earth 2001 <a href="https://en.wikipedia.org/wiki/Google_Earth" class="uri">https://en.wikipedia.org/wiki/Google_Earth</a></li>
<li>OSM 2004 <a href="https://en.wikipedia.org/wiki/OpenStreetMap" class="uri">https://en.wikipedia.org/wiki/OpenStreetMap</a></li>
<li>Google map 2005 <a href="https://en.wikipedia.org/wiki/Google_Maps" class="uri">https://en.wikipedia.org/wiki/Google_Maps</a></li>
<li>Twitter 2006 <a href="https://en.wikipedia.org/wiki/Twitter" class="uri">https://en.wikipedia.org/wiki/Twitter</a></li>
<li>GSV 2007 <a href="https://en.wikipedia.org/wiki/Google_Street_View" class="uri">https://en.wikipedia.org/wiki/Google_Street_View</a></li>
<li>Mapillary 2013 <a href="https://en.wikipedia.org/wiki/Mapillary" class="uri">https://en.wikipedia.org/wiki/Mapillary</a></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="introduction-1" class="slide level2 smaller">
<h2>Introduction</h2>
<div class="cell" data-reveal="true" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<pre class="mermaid mermaid-js">timeline
    title  My research: imagery &amp; GIS
    1998-2001: PhD 
             : Road scene video analysis
    2014-2018: GraiSearch  (FP7)
             : Social media       
    2016-2020: Bonseyes (H2020)
             : A.I. CNN
    2017: Eir 
        : Telegraph poles in ROI
        : road scene imagery (GSV)
    2018: OSI 
        : aerial imagery
        : roof tops
    2019-.: aimapit.com
        : collaborator
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<!-- -->
</section>
<section id="introduction-2" class="slide level2 smaller">
<h2>Introduction</h2>
<p>Motivations:</p>
<ul>
<li>Infrastructure maintenance</li>
<li>Infrastructure compliance (safety)</li>
<li>Planning (e.g.&nbsp;EV public chargers deployment)</li>
<li>Autonomous robotics</li>
<li>Bio diversity monitoring</li>
<li>etc.</li>
</ul>
<p>Focus of this talk:</p>
<ul>
<li><code>Static Object</code> geotagging e.g.&nbsp;traffic signs, poles, trees (but <span class="math inline">\(\neq\)</span> cars or pedestrians)</li>
</ul>
<p><img data-src="images/IEEEWISP2025/PoleInspection.jpg" class="absolute" style="top: 0px; left: 700px; width: 350px; "></p>
<!-- -->
</section>
<section id="early-works" class="slide level2 smaller">
<h2>Early works</h2>

<img data-src="images/IEEEWISP2025/ICIP2001.svg" class="r-stretch quarto-figure-center"><p class="caption">Detection of Changing Objects in Camera-in-Motion Video</p><div class="footer">
<p>DOI:10.1109/<strong>ICIP.2001</strong>.959126 <span class="citation" data-cites="Dahyot_icip01">(<a href="#/references" role="doc-biblioref" onclick="">Dahyot, Charbonnier, and Heitz 2001</a>)</span></p>
</div>
<aside class="notes">
<ul>
<li><p>Interest: static objects on the side of the road (trees, traffic signs, poles,…)</p></li>
<li><p>images captured every 5 meters: frontal view from car roof (monocular camera)</p></li>
<li><p>no GPS available! but milestones <a href="https://en.wikipedia.org/wiki/Milestone" class="uri">https://en.wikipedia.org/wiki/Milestone</a> used as geolocation references</p></li>
<li><p>feature distribution comparisons</p>
<ul>
<li><span class="math inline">\(p(m|t)\)</span> Vs <span class="math inline">\(p(m|t-1)\)</span></li>
<li><span class="math inline">\(p(m|t)\)</span> Vs <span class="math inline">\(p(m|t+1)\)</span></li>
</ul></li>
<li><p>extraction of changing feature distributions <span class="math inline">\(p(m| t \text{ vs } t-1)\)</span> and <span class="math inline">\(p(m| t \text{ vs } t+1)\)</span> for backprojections.</p></li>
<li><p>features <span class="math inline">\(m\)</span> can be pixel colour (e.g.&nbsp;chrominance pixel pairs (2D)) and pixel shape information (Hough Transform based features)</p></li>
<li><p>good detection rate, unsupervised technique!</p></li>
<li><p>but sensitive to other events causing visual changes (false alarms) e.g.&nbsp;cars overtaking, etc.<br>
</p></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<!-- -->
</section>
<section id="early-works-1" class="slide level2 smaller">
<h2>Early works</h2>
<div class="columns">
<div class="column" style="width:40%;">
<p><span class="math inline">\(\Rightarrow\)</span> low dimensional feature engineering (straight edges)</p>
<p><span class="math inline">\(\Rightarrow\)</span> pixel positions <span class="math inline">\(\color{green}{(x_i,y_i)}\)</span> used in feature computation (<span class="math inline">\(\cong\)</span> positional encoding)</p>
<p><span class="math inline">\(\Rightarrow\)</span> P.d.f. modelled with histograms or KDE.</p>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:50%;">
<p><strong>Shape descriptors</strong> at pixel <span class="math inline">\(i\)</span> with position <span class="math inline">\((x_i,y_i)\)</span> in image <span class="math inline">\(I\)</span> with derivatives <span class="math inline">\(I_x\)</span> and <span class="math inline">\(I_y\)</span>: <span class="math display">\[
\left\lbrace
\begin{array}{l}
\|\nabla I_i\|=\sqrt{I^2_x(x_i,y_i)+I^2_y(x_i,y_i)}\\
\theta_i=\arctan\left( \frac{I_y(x_i,y_i)}{I_x(x_i,y_i)}\right)\\
\rho_i= \color{green}{x_i} \cdot \frac{I_x(x_i,y_i)}{\|\nabla I_i\|} + \color{green}{y_i} \cdot \frac{I_y(x_i,y_i)}{\|\nabla I_i\|}
\end{array}
\right.
\]</span> Hough Transform estimate: <span class="math display">\[
\hat{p}(\theta,\rho)=\frac{1}{N}\sum_{i=1}^N \frac{1}{h_{\theta_i}} k_{\theta}\left(\frac{\theta-\theta_i}{h_{\theta_i}}\right)\ R_i(\theta,\rho)
\]</span></p>
</div></div>
<div class="footer">
<p>DOI:10.1109/<strong>TPAMI.2008</strong>.288 <span class="citation" data-cites="Dahyot08pami">(<a href="#/references" role="doc-biblioref" onclick="">Dahyot 2009</a>)</span></p>
</div>
<aside class="notes">
<ul>
<li><p>Note the usage of the pixel coordinates in the computation of feature <span class="math inline">\(\rho\)</span> (<span class="math inline">\(\cong\)</span> positional encoding)</p></li>
<li><p>Hough Transform = vintage Computer vision <a href="https://en.wikipedia.org/wiki/Hough_transform" class="uri">https://en.wikipedia.org/wiki/Hough_transform</a></p></li>
<li><p>Radon transform use to compute the kernel <span class="math inline">\(R_i\)</span>: <span class="math display">\[
R_i(\theta,\rho)= \int\int \delta(\rho-x\cos \theta-y\sin \theta)\ k(x-x_i)\ k(y-y_i)\ dx\ dy
\]</span></p></li>
<li><p>Many man-made objects have straight edges (tree trunks sometimes have these too)</p></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<!-- -->
</section>
<section id="early-works-2" class="slide level2 smaller">
<h2>Early works</h2>

<img data-src="images/IEEEWISP2025/CVPR2000.svg" class="r-stretch quarto-figure-center"><p class="caption">Robust Object Recognition</p><div class="footer">
<p>DOI:10.1109/<strong>CVPR.2000</strong>.855886 <span class="citation" data-cites="Dahyot_cvpr00">(<a href="#/references" role="doc-biblioref" onclick="">Dahyot, Charbonnier, and Heitz 2000</a>)</span></p>
</div>
<aside class="notes">
<p><span class="citation" data-cites="Dahyot_cvpr00">(<a href="#/references" role="doc-biblioref" onclick="">Dahyot, Charbonnier, and Heitz 2000</a>)</span> proposed:</p>
<ul>
<li><p>using supervised learning with a (simple) AI (PCA) using a clean dataset of traffic signs augmented with rotations.</p></li>
<li><p>at test time though, out-of-distribution observations that deviate from training set need to be robustly detected and recognised.</p></li>
<li><p>this was done by performing robust regression against the Principal Components using a robust loss (M-estimator loss) at test time.</p></li>
<li><p>a sequence of robust loss functions was used to progressively increase the strength by which outlier pixels are rejected. See the mask image provided as a <strong>feedback 1</strong> as a by product to the robust regression: the white pixel (weight value close to 1) are inliers, while black pixel (weight value close to 0) are the detected outlier pixels.</p></li>
<li><p>For a more recent overview of these robust loss functions, read also <span class="citation" data-cites="8954089">(<a href="#/references" role="doc-biblioref" onclick="">Barron 2019</a>, CVPR 2019)</span></p></li>
<li><p>the slide presents two results where our “AI” is successful (when occluding pixels are in blue) and is unsuccessful (when the same occluding pixels are now black leading to confusion to another traffic sign being recognised)</p></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<!-- -->
</section>
<section id="early-works-3" class="slide level2 smaller">
<h2>Early works</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/IEEEWISP2025/PAA2004.svg"></p>
<figcaption>Robust Object Detection</figcaption>
</figure>
</div>
</div><div class="column" style="width:50%;">
<p>Extension to object detection:</p>
<ul>
<li>several robust scores (based M-estimators) computed with a sliding window</li>
<li>Bayesian probability interpretation of these robust scores.</li>
<li>method robust to partial occlusion and cluttered background</li>
</ul>
</div></div>
<div class="footer">
<p>Pattern Analysis &amp; Applications DOI:10.1007/s10044-004-0230-5 <span class="citation" data-cites="Dahyot_PAA">(<a href="#/references" role="doc-biblioref" onclick="">Dahyot, Charbonnier, and Heitz 2004</a>)</span></p>
</div>
<aside class="notes">
<p><span class="citation" data-cites="Dahyot_PAA">(<a href="#/references" role="doc-biblioref" onclick="">Dahyot, Charbonnier, and Heitz 2004</a>)</span> proposed:</p>
<ul>
<li>Using a sliding window, the robust recognition framework is extended to detection in an image.</li>
<li>the robust heat map shows one of robust similarity metrics we have proposed using M-estimators:
<ul>
<li><p>the black pixels indicates high similarity with training images</p></li>
<li><p>the white pixels indicates low similarity with training images</p></li>
<li><p>the purple pixels indicates location where no similarity is computed as the sliding window would be step outside the scene image analysed (no padding used!)</p></li>
<li><p>note that even with a cluttered background and a partial occlusion, the object is well detected!</p></li>
</ul></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<!-- -->
</section>
<section id="early-works-4" class="slide level2 smaller">
<h2>Early works</h2>
<div class="columns">
<div class="column" style="width:75%;">
<iframe data-external="1" src="https://www.youtube.com/embed/NxPeNXV4b-A?start=156" width="700" height="500" title="Visualising popular tweet spots in 3D" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</div><div class="column" style="width:25%;">
<p>Viz Unreal engine:</p>
<ul>
<li>NN score sentiment analysis of <code>tweet text</code> (colour lights: blue=sad; yellow=happy)</li>
<li><code>tweet image</code> geolocation+orientation inferred by feature matching against GSV to find <code>tweet image</code> GPS and orientation<br>
</li>
<li>3D city construction using OSM and GSV data.</li>
</ul>
</div></div>
<div class="footer">
<p>DOI:10.1016/j.cag.2017.01.005 <span class="citation" data-cites="BulbulCAG2016">(<a href="#/references" role="doc-biblioref" onclick="">Bulbul and Dahyot 2017</a>)</span></p>
</div>
<aside class="notes">
<p><span class="citation" data-cites="BulbulCAG2016">(<a href="#/references" role="doc-biblioref" onclick="">Bulbul and Dahyot 2017</a>)</span> uses:</p>
<ul>
<li>Twitter data: a Neural Network was used for sentiment analysis of text,</li>
<li>the geolocation of the tweet and by analysing the image content, the 6D camera pose is estimated (Geolocation of image and direction of view )</li>
<li>3D city reconstruction using:
<ul>
<li>OSM building footprint and building height</li>
<li>GSV building facade image are segmented and sticked to 3D building</li>
</ul></li>
<li>Rendering in Unreal engine
<ul>
<li>each tweet correspond to a light coloured by its text sentiment score (blue sad/yellow happy) and directed using the direction of the photograph in the tweet.</li>
</ul></li>
<li>In the results for Rome, tweets focuses on tourist landmarks such as the <a href="https://maps.app.goo.gl/1HcQDcs4aKrs5UZA6" target="_blank">Pantheon</a> and <a href="https://maps.app.goo.gl/Uh71XrB7jb9FtJr26">Monument to Victor Emmanuel II</a>.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<!-- -->
</section>
<section id="geotagging-of-objects" class="slide level2 smaller">
<h2>Geotagging of Objects</h2>

<img data-src="images/IEEEWISP2025/aimapit-pipeline.svg" class="r-stretch"><div class="footer">
<p>Remote Sensing DOI:10.3390/rs10050661 <span class="citation" data-cites="Krylov_2018">(<a href="#/references" role="doc-biblioref" onclick="">V. Krylov, Kenny, and Dahyot 2018</a>)</span></p>
</div>
<aside class="notes">
<p>In <span class="citation" data-cites="Krylov_2018">(<a href="#/references" role="doc-biblioref" onclick="">V. Krylov, Kenny, and Dahyot 2018</a>)</span> we propose:</p>
<ul>
<li><p>a Markov Random Field to merge all information and to infer unique GPS tags for the objects of interest.</p></li>
<li><p>note that an object can appear multiple times across several images: the output is a unique GPS for each object (no duplicate)</p></li>
<li><p>we chose a <code>modular</code> approach: each module is trained/fine tuned independently.</p></li>
<li><p>the modular approach allows to use already trained modules (e.g.&nbsp;depth estimation)</p></li>
<li><p>We fine tune the image segmentation module to focus on our object of interest (telegraph poles)</p></li>
<li><p>Alternative approach: <code>End-to-End</code> learnable approach <span class="citation" data-cites="GeoGraphECCV2020">(<a href="#/references" role="doc-biblioref" onclick="">Nassar et al. 2020</a>, GeoGraph)</span> has that uses a GNN in combination to CNNs</p></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<!-- -->
</section>
<section id="geotagging-of-objects-1" class="slide level2 smaller">
<h2>Geotagging of Objects</h2>

<img data-src="images/IEEEWISP2025/RS2018.svg" class="r-stretch"><div class="footer">
<p>Remote Sensing DOI:10.3390/rs10050661 <span class="citation" data-cites="Krylov_2018">(<a href="#/references" role="doc-biblioref" onclick="">V. Krylov, Kenny, and Dahyot 2018</a>)</span></p>
</div>
<aside class="notes">
<p>In <span class="citation" data-cites="Krylov_2018">(<a href="#/references" role="doc-biblioref" onclick="">V. Krylov, Kenny, and Dahyot 2018</a>)</span>:</p>
<ul>
<li>the Object of interest are telegraph poles</li>
<li>Data: GSV images</li>
<li>Deep Neural Networks: Convolutional Neural Network</li>
<li>Poles cannot be well detected from aerial/satellite images (too thin) and Street View imagery is more suited for that task.</li>
<li>A pretrained CNN is used for depth estimation as multiple view feature matching approaches dont work well with thin symmetric objects like poles.</li>
<li>The CNN is trained for object segmentation</li>
<li>MRF for fusion of information object GPS inference</li>
<li>ROI (Region of Interest): ROI (Republic of Ireland!). The whole country was processed with one GPU boosted PC by decomposing the country into small overlapping areas.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<!-- -->
</section>
<section id="geotagging-of-objects-2" class="slide level2 smaller">
<h2>Geotagging of Objects</h2>
<p>MRF energy to minimize is formulated as: <span class="math display">\[
\mathcal{U}(\mathbf{z}=\lbrace z_1,\cdots,z_{N_{\mathcal{Z}}}\rbrace,\lbrace\alpha\rbrace)=\sum_{i=1}^{N_{\mathcal{Z}}} \sum_{j} \alpha_j \ \underbrace{u_j(z_i) }_{\text{energy term } j}
\]</span></p>
<ul>
<li>Each image detection(s) corresponds to a ray with origin camera GPS and direction derived from pixel location and camera orientation</li>
<li>Any pair of intersecting rays (from pair of images) define a site (intersection) <span class="math inline">\(i\)</span> as potential candidate for object geolocation. <span class="math inline">\(N_{\mathcal{Z}}\)</span> is the total number of candidate sites extracted from a collection of images.<br>
</li>
<li><span class="math inline">\(z_i\)</span> at site <span class="math inline">\(i\)</span> is a binary variable:
<ul>
<li><span class="math inline">\(z_i=0\)</span> : <code>object is absent</code></li>
<li><span class="math inline">\(z_i=1\)</span> : <code>object is present</code></li>
</ul></li>
</ul>
<div class="footer">
<p>Remote Sensing DOI:10.3390/rs10050661 <span class="citation" data-cites="Krylov_2018">(<a href="#/references" role="doc-biblioref" onclick="">V. Krylov, Kenny, and Dahyot 2018</a>)</span></p>
</div>
<aside class="notes">
<p>In <span class="citation" data-cites="Krylov_2018">(<a href="#/references" role="doc-biblioref" onclick="">V. Krylov, Kenny, and Dahyot 2018</a>)</span>:</p>
<ul>
<li>rays = half lines with starting origin camera GPS location.</li>
<li>the sites of the MRF are all the intersections between 2 rays emerging from 2 camera views.</li>
<li>These MRF sites define an <code>irregular grid</code>.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<!-- -->
</section>
<section id="geotagging-of-objects-3" class="slide level2 smaller">
<h2>Geotagging of Objects</h2>
<p><strong>Energy terms <span class="math inline">\(u_j(\cdot)\)</span>:</strong></p>
<ul>
<li><p>One that enforces consistency with the depth estimation.</p></li>
<li><p>A pairwise energy term that depends on the current state <span class="math inline">\(z\)</span> and those of its neighbours <span class="math inline">\(\lbrace z_k \rbrace\)</span>, is introduced to enforce one detection from multiple sites clustered together.</p></li>
<li><p>An energy term penalizes rays that have no positive intersections: false positives or objects discovered from a single camera position.</p></li>
</ul>
<p><strong>MRF optimization:</strong></p>
<ul>
<li>Energy minimization computed with an iterative conditional modes (ICM) algorithm<br>
</li>
<li>Initial state for ICM is set as <span class="math inline">\(z_i=0,\forall i\)</span> (all sites are empty)</li>
</ul>
<div class="footer">
<p>Remote Sensing DOI:10.3390/rs10050661 <span class="citation" data-cites="Krylov_2018">(<a href="#/references" role="doc-biblioref" onclick="">V. Krylov, Kenny, and Dahyot 2018</a>)</span></p>
</div>
<aside class="notes">

<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<!-- -->
</section>
<section id="geotagging-of-objects-4" class="slide level2 smaller">
<h2>Geotagging of Objects</h2>
<p><strong>Preprocessing:</strong></p>
<ul>
<li>Clustering: Ray intersections are only considered when in a max distance of 25 meters from the origins of the rays (camera GPS)</li>
</ul>
<p><strong>Postprocessing: </strong></p>
<ul>
<li>Clustering: Locations of positive sites found in the same vicinity (radius=1 meter) are averaged to obtain the final unique object’s geotag.</li>
</ul>
<div class="footer">
<p>Remote Sensing DOI:10.3390/rs10050661 <span class="citation" data-cites="Krylov_2018">(<a href="#/references" role="doc-biblioref" onclick="">V. Krylov, Kenny, and Dahyot 2018</a>)</span></p>
</div>
<aside class="notes">
<ul>
<li>Postprocessing step refines the final GPS location reported in the list.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<!-- -->
</section>
<section id="geotagging-of-objects-5" class="slide level2 smaller">
<h2>Geotagging of Objects</h2>
<p><strong>Evaluation:</strong></p>
<ul>
<li>Object Detection</li>
</ul>

<img data-src="images/IEEEWISP2025/RS2018-Tab1.png" class="r-stretch"><div class="footer">
<p>Remote Sensing DOI:10.3390/rs10050661 <span class="citation" data-cites="Krylov_2018">(<a href="#/references" role="doc-biblioref" onclick="">V. Krylov, Kenny, and Dahyot 2018</a>)</span></p>
</div>
<aside class="notes">
<p><a href="https://en.wikipedia.org/wiki/Precision_and_recall" target="_blank">Definitions</a></p>
<ul>
<li><p>precision= relevant retrieved instances /all <strong>retrieved</strong> instances</p></li>
<li><p>recall= relevant retrieved instances / all <strong>relevant</strong> instances</p></li>
<li><p>experiments on traffic lights and telegraph poles.</p></li>
<li><p>for traffic lights, there are 50 to be found, 51 instance were detected including 47 actual traffic lights (TP), 4 false positive (not traffic lights). 3 traffic lights were missed (FN).</p></li>
<li><p>High recall/precision (close to 1) show good performance.</p></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<!-- -->
</section>
<section id="geotagging-of-objects-6" class="slide level2 smaller">
<h2>Geotagging of Objects</h2>
<p><strong>Evaluation:</strong></p>
<ul>
<li>Absolute GPS positions against ground truth</li>
</ul>

<img data-src="images/IEEEWISP2025/RS2018-Tab2.png" class="r-stretch"><div class="footer">
<p>Remote Sensing DOI:10.3390/rs10050661 <span class="citation" data-cites="Krylov_2018">(<a href="#/references" role="doc-biblioref" onclick="">V. Krylov, Kenny, and Dahyot 2018</a>)</span></p>
</div>
<aside class="notes">
<p>In <span class="citation" data-cites="Krylov_2018">(<a href="#/references" role="doc-biblioref" onclick="">V. Krylov, Kenny, and Dahyot 2018</a>)</span>:</p>
<ul>
<li><p>robust registration between detected locations and ground truth locations</p></li>
<li><p>Note: Ground truth GPS difficult to get.</p></li>
<li><p>importance of MRF for improving Geolocation (depth CNN is not enough).</p></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<!-- -->
</section>
<section id="geotagging-of-objects-7" class="slide level2 smaller">
<h2>Geotagging of Objects</h2>
<div class="columns">
<div class="column" style="width:75%;">
<iframe data-external="1" src="https://www.youtube.com/embed/iQhvHW1DqB0?start=70" width="700" height="500" title="AImapit 2019" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</div><div class="column" style="width:25%;">

</div></div>
<div class="footer">
<p>Remote Sensing DOI:10.3390/rs10050661 <span class="citation" data-cites="Krylov_2018">(<a href="#/references" role="doc-biblioref" onclick="">V. Krylov, Kenny, and Dahyot 2018</a>)</span></p>
</div>
<aside class="notes">
<p>This is a video on youtube demostrating the approach <span class="citation" data-cites="Krylov_2018">(<a href="#/references" role="doc-biblioref" onclick="">V. Krylov, Kenny, and Dahyot 2018</a>)</span>.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<!-- -->
</section>
<section id="geotagging-of-objects-8" class="slide level2 smaller">
<h2>Geotagging of Objects</h2>
<div class="columns">
<div class="column" style="width:75%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/IEEEWISP2025/LidarDublin2015.png"></p>
<figcaption>Traffic lights geolocation using GSV + Lidar <span class="citation" data-cites="DublinLidar2015">(<a href="#/references" role="doc-biblioref" onclick="">Laefer et al. 2017</a>)</span></figcaption>
</figure>
</div>
</div><div class="column" style="width:25%;">
<ul>
<li>Detection in Lidar point cloud performed by template matching using a pole-like object template (false alarm rate: high).</li>
<li>The MRF energy is modified by adding a new term to take into account Lidar candidate locations near each of the MRF sites.</li>
</ul>
</div></div>
<div class="footer">
<p>DOI:10.1109/<strong>ICIP.2018</strong>.8451458 <span class="citation" data-cites="Krylov_ICIP2018">(<a href="#/references" role="doc-biblioref" onclick="">V. A. Krylov and Dahyot 2018</a>)</span></p>
</div>
<aside class="notes">
<ul>
<li>GSV images processed with fully convolutional neural networks (FCNNs)
<ul>
<li>sementic segmentation</li>
<li>monocular depth estimation</li>
</ul></li>
<li>the MRF irregular grid of sites are found as before with pair of rays coming from image segmentation</li>
<li>LiDar: Detection of potential object locations is addressed as a template matching using a pole-like object template (free-standing object with height [2,6] meters above the ground level).</li>
<li>validated for traffic lights detection.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<!-- -->
</section>
<section id="geotagging-of-objects-9" class="slide level2 smaller">
<h2>Geotagging of Objects</h2>

<img data-src="images/IEEEWISP2025/ECML2019.png" class="r-stretch quarto-figure-center"><p class="caption">Evaluation was also performed with <strong>Mapillary</strong> crowdsourced images (bottom pipeline) instead of GSV (top pipeline for reference). Additional preprocessing is needed to inferred the missing image metadata in Mapilliary.</p><div class="footer">
<p>ECML workshop 2019 <span class="citation" data-cites="10.1007/978-3-030-13453-2_7">(<a href="#/references" role="doc-biblioref" onclick="">V. A. Krylov and Dahyot 2019</a>)</span></p>
</div>
<aside class="notes">
<p>From <span class="citation" data-cites="10.1007/978-3-030-13453-2_7">(<a href="#/references" role="doc-biblioref" onclick="">V. A. Krylov and Dahyot 2019</a>)</span></p>
<ul>
<li>Due to excessive camera position noise, the estimated geolocations are less accurate when using Mapilliary than with using GSV.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<!-- -->
</section>
<section id="geotagging-of-objects-10" class="slide level2 smaller">
<h2>Geotagging of Objects</h2>
<ul>
<li><strong>Pre-processing</strong>: Enhancing quality of the GSV image metadata using Structure from Motion (Camera translation T and/or rotation R estimation).</li>
<li><strong>Postprocessing:</strong> predicted object geolocation is further refined by imposing contextual geographic information extracted from OSM.</li>
</ul>
<div style="font-size: 50%;">
<table class="caption-top">
<colgroup>
<col style="width: 15%">
<col style="width: 5%">
<col style="width: 6%">
<col style="width: 2%">
<col style="width: 7%">
<col style="width: 5%">
<col style="width: 7%">
<col style="width: 23%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th>Image metadata correction</th>
<th>Actual</th>
<th>Detected</th>
<th>TP</th>
<th>Precision<span class="math inline">\(\uparrow\)</span></th>
<th>Recall<span class="math inline">\(\uparrow\)</span></th>
<th>F-measure<span class="math inline">\(\uparrow\)</span></th>
<th>Error in meters <span class="math inline">\(\downarrow\)</span></th>
<th>Error in meters (with OSM) <span class="math inline">\(\downarrow\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>None</td>
<td>76</td>
<td>94</td>
<td>58</td>
<td>0.61</td>
<td><strong>0.76</strong></td>
<td>0.68</td>
<td>2.71</td>
<td>2.64</td>
</tr>
<tr class="even">
<td>T</td>
<td>76</td>
<td>89</td>
<td>57</td>
<td><strong>0.64</strong></td>
<td>0.75</td>
<td><strong>0.69</strong></td>
<td>2.79</td>
<td>2.74</td>
</tr>
<tr class="odd">
<td>R and T</td>
<td>76</td>
<td>92</td>
<td>54</td>
<td>0.57</td>
<td>0.72</td>
<td>0.64</td>
<td><strong>2.53</strong></td>
<td><strong>2.48</strong></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<div class="footer">
<p>IMVIP 2021 <span class="citation" data-cites="ChaoImvip2021">(<a href="#/references" role="doc-biblioref" onclick="">Liu et al. 2021</a>)</span> + patent 2023 <span class="citation" data-cites="Patent2023">(<a href="#/references" role="doc-biblioref" onclick="">Liu, Ulicny, and Dahyot 2023</a>)</span></p>
</div>
<aside class="notes">
<p>In <span class="citation" data-cites="ChaoImvip2021">(<a href="#/references" role="doc-biblioref" onclick="">Liu et al. 2021</a>)</span> - we further refine the pipeline with the aim to improve data input quality from GSV (bundle adjustment) and further constraint the geolocation estimates using context information from OSM.</p>
<ul>
<li><p>note that with OSM, GPS location improves with or without bundle adjustment</p></li>
<li><p>Definition <a href="https://en.wikipedia.org/wiki/F-score" target="_blank">F-score:</a> The highest possible value of an F-score is 1.0, indicating perfect precision and recall, and the lowest possible value is 0, if the precision or the recall is zero.</p></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<!-- -->
</section>
<section id="geotagging-of-objects-11" class="slide level2 smaller">
<h2>Geotagging of Objects</h2>

<img data-src="images/IEEEWISP2025/aimapit2025.png" class="r-stretch quarto-figure-center"><p class="caption">Research out of the lab!</p><div class="footer">
<p><a href="https://www.aimapit.com/" target="_blank">aimapit.com</a></p>
</div>
<!-- -->
</section>
<section id="improving-dnns" class="slide level2 smaller">
<h2>Improving DNNs</h2>
<div class="columns">
<div class="column" style="width:70%;">
<p>In CNN, filter weights <span class="math inline">\(\lbrace \alpha_i\rbrace\)</span> are learnt: <span class="math display">\[
   \text{filter for convolution}=\sum_i \alpha_i \ \mathbf{e}_i  \\ \text{with} \ \lbrace \mathbf{e}_i\rbrace \equiv \text{natural basis}
\]</span></p>
<p>We propose <strong>Harmonic CNN layer</strong>, where the <strong>natural basis is replaced by DCT basis</strong>,that:</p>
<ul>
<li>replace conventional convolutional layers to produce harmonic versions of existing CNN architectures,</li>
<li>can be efficiently compressed by truncating high-frequency components,</li>
<li>has been validated extensively for image classification, object detection and semantic segmentation applications.</li>
</ul>
</div><div class="column" style="width:30%;">
<p><img data-src="images/IEEEWISP2025/DCT.png"></p>
</div></div>
<div class="footer">
<p>DOI:10.1016/j.patcog.2022.108707 <span class="citation" data-cites="ULICNY2022108707">(<a href="#/references" role="doc-biblioref" onclick="">Ulicny, Krylov, and Dahyot 2022</a>)</span></p>
</div>
<aside class="notes">
<ul>
<li>work by Matej ulicny PhD TCD</li>
<li>focus on CNN</li>
<li>started out with looking at applying CNN to JPEG images (that use DCT).</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<!-- -->
</section>
<section id="improving-dnns-1" class="slide level2">
<h2>Improving DNNs</h2>
<div class="columns">
<div class="column" style="width:70%;">
<p><img data-src="images/IEEEWISP2025/JeremyPhD.png"></p>
</div><div class="column" style="width:30%;">
<div style="font-size: 50%;">
<p>Graph matching as postprocessing of DNN segmentation results. Example for 3D segmentation of brain (IBSR dataset):</p>
</div>
<video id="video_shortcode_videojs_video1" width="300%" class="video-js vjs-default-skin " controls="" preload="auto" data-setup="{}" title=""><source src="./images/IEEEWISP2025/Brain.mp4"></video>
</div></div>
<div class="footer">
<p>DOI:10.1016/j.<strong>cviu.2023</strong>.103744 <span class="citation" data-cites="CHOPIN2023103744">(<a href="#/references" role="doc-biblioref" onclick="">Chopin et al. 2023</a>)</span></p>
</div>
<aside class="notes">
<p>About <span class="citation" data-cites="CHOPIN2023103744">(<a href="#/references" role="doc-biblioref" onclick="">Chopin et al. 2023</a>)</span></p>
<ul>
<li>work by Jeremy Chopin, PhD thesis Angers University, Postdoc MU.<br>
</li>
<li>adding graph matching to correct error of segmentation from a CNN or transformer as a post process</li>
<li>efficient when small number of samples available for training.</li>
<li>IBSR dataset 18 brain 3D MRI with 32 regions annotated : only 14 considered in our evaluation</li>
<li>structural information is encoded in our fully connected graph</li>
<li>graph matching correct oversegmentation (but not undersegmentation), and mislabelled region</li>
<li>graph matching can be solved with reinforcement learning <span class="citation" data-cites="ChopinICPRAI2022b">(<a href="#/references" role="doc-biblioref" onclick="">Chopin et al. 2022</a>)</span></li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<!-- -->
</section>
<section id="summary" class="slide level2 smaller">
<h2>Summary</h2>
<p>We presented a modular pipeline for object geotagging:</p>
<ul>
<li><p>Each module is optimised individually (not end-to-end).</p></li>
<li><p>Image segmentation and depth estimation are performed with DNNs</p></li>
<li><p>DNN based modules have been updated overtime (i.e.&nbsp;architecture changes).</p></li>
<li><p>Data quality is very important (e.g.&nbsp;metadata) for geotagging accuracy.</p></li>
<li><p>For creating a new training dataset for a new object of interest, we have taken advantages of multiple approaches (e.g.&nbsp;vintage Computer Vision approach, or AIs e.g.&nbsp;<a href="https://github.com/IDEA-Research/Grounded-Segment-Anything" target="_blank">SAM</a>)</p></li>
<li><p>MRF provides a flexible formalism to take advantage of multiple sources of information.</p></li>
</ul>
<aside class="notes">
<ul>
<li>image examples of object of interest need to be provided for supervised learning</li>
<li>sometimes it also means to gather images of objects that are not of interest but similar (e.g.&nbsp;electric poles Vs telegraphic poles) to improve robustness of the DNNs to false positive.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="thank-you-any-questions" class="slide level2 smaller">
<h2>Thank you! Any Questions?</h2>
<div class="columns">
<div class="column" style="width:75%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/IEEEWISP2025/aimapitTeam2023.jpg"></p>
<figcaption>Picture left to right: V. Krylov, R. Dahyot, J. Connelly &amp; M. Ulicny (2023)</figcaption>
</figure>
</div>
</div><div class="column" style="width:25%;">
<p>Many thanks to all my collaborators, past and present!</p>
</div></div>
<div class="footer">
<p>Check out V. Krylov’s most recent paper on roadside object geolocation <span class="citation" data-cites="10715092">(<a href="#/references" role="doc-biblioref" onclick="">Ahmad and Krylov 2024</a>, EUSIPCO 2024)</span></p>
</div>
</section>
<section id="references" class="slide level2 smaller scrollable">
<h2>References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-10715092" class="csl-entry" role="listitem">
Ahmad, W., and V. A. Krylov. 2024. <span>“Roadside Object Geolocation from Street-Level Images with Reduced Supervision.”</span> In <em>2024 32nd European Signal Processing Conference (EUSIPCO)</em>, 641–45. <a href="https://doi.org/10.23919/EUSIPCO63174.2024.10715092">https://doi.org/10.23919/EUSIPCO63174.2024.10715092</a>.
</div>
<div id="ref-8954089" class="csl-entry" role="listitem">
Barron, J. T. 2019. <span>“A General and Adaptive Robust Loss Function.”</span> In <em>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 4326–34. <a href="https://doi.org/10.1109/CVPR.2019.00446">https://doi.org/10.1109/CVPR.2019.00446</a>.
</div>
<div id="ref-BulbulCAG2016" class="csl-entry" role="listitem">
Bulbul, A., and R. Dahyot. 2017. <span>“Social Media Based 3D Visual Popularity.”</span> <em>Computers &amp; Graphics</em> 63: 28–36. <a href="https://doi.org/10.1016/j.cag.2017.01.005">https://doi.org/10.1016/j.cag.2017.01.005</a>.
</div>
<div id="ref-ChopinICPRAI2022b" class="csl-entry" role="listitem">
Chopin, J., J.-B. Fasquel, H. Mouchere, R. Dahyot, and I. Bloch. 2022. <span>“QAP Optimisation with Reinforcement Learning for Faster Graph Matching in Sequential Semantic Image Analysis.”</span> In <em>Pattern Recognition and Artificial Intelligence</em>, edited by Mounîm El Yacoubi, Eric Granger, Pong Chi Yuen, Umapada Pal, and Nicole Vincent, 47–58. Paris, France: Springer International Publishing. <a href="https://doi.org/10.1007/978-3-031-09037-0_5">https://doi.org/10.1007/978-3-031-09037-0_5</a>.
</div>
<div id="ref-CHOPIN2023103744" class="csl-entry" role="listitem">
Chopin, J., J.-B. Fasquel, H. Mouchère, R. Dahyot, and I. Bloch. 2023. <span>“Model-Based Inexact Graph Matching on Top of DNNs for Semantic Scene Understanding.”</span> <em>Computer Vision and Image Understanding</em>, 103744. <a href="https://doi.org/10.1016/j.cviu.2023.103744">https://doi.org/10.1016/j.cviu.2023.103744</a>.
</div>
<div id="ref-Dahyot08pami" class="csl-entry" role="listitem">
Dahyot, R. 2009. <span>“Statistical Hough Transform.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, no. 8 (August): 1502–9. <a href="https://doi.org/10.1109/TPAMI.2008.288">https://doi.org/10.1109/TPAMI.2008.288</a>.
</div>
<div id="ref-Dahyot_cvpr00" class="csl-entry" role="listitem">
Dahyot, R., P. Charbonnier, and F. Heitz. 2000. <span>“Robust Visual Recognition of Colour Images.”</span> In <em>Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 1:685–690 vol.1. <a href="https://doi.org/10.1109/CVPR.2000.855886">https://doi.org/10.1109/CVPR.2000.855886</a>.
</div>
<div id="ref-Dahyot_icip01" class="csl-entry" role="listitem">
———. 2001. <span>“Unsupervised Statistical Detection of Changing Objects in Camera-in-Motion Video.”</span> In <em>Proceedings 2001 International Conference on Image Processing</em>, 1:638–41. <a href="https://doi.org/10.1109/ICIP.2001.959126">https://doi.org/10.1109/ICIP.2001.959126</a>.
</div>
<div id="ref-Dahyot_PAA" class="csl-entry" role="listitem">
———. 2004. <span>“A Bayesian Approach to Object Detection Using Probabilistic Appearance-Based Models.”</span> <em>Pattern Analysis and Applications</em> 7 (3): 317–32. <a href="https://doi.org/10.1007/s10044-004-0230-5">https://doi.org/10.1007/s10044-004-0230-5</a>.
</div>
<div id="ref-Krylov_ICIP2018" class="csl-entry" role="listitem">
Krylov, V. A., and R. Dahyot. 2018. <span>“Object Geolocation Using MRF Based Multi-Sensor Fusion.”</span> In <em>2018 25th IEEE International Conference on Image Processing (ICIP)</em>, 2745–49. <a href="https://doi.org/10.1109/ICIP.2018.8451458">https://doi.org/10.1109/ICIP.2018.8451458</a>.
</div>
<div id="ref-10.1007/978-3-030-13453-2_7" class="csl-entry" role="listitem">
———. 2019. <span>“Object Geolocation from Crowdsourced Street Level Imagery.”</span> In <em>ECML PKDD 2018 Workshops</em>, edited by Carlos Alzate, Anna Monreale, Haytham Assem, Albert Bifet, Teodora Sandra Buda, Bora Caglayan, Brett Drury, et al., 79–83. Cham: Springer International Publishing. <a href="https://doi.org/10.1007/978-3-030-13453-2_7">https://doi.org/10.1007/978-3-030-13453-2_7</a>.
</div>
<div id="ref-Krylov_2018" class="csl-entry" role="listitem">
Krylov, V., E. Kenny, and R. Dahyot. 2018. <span>“Automatic Discovery and Geotagging of Objects from Street View Imagery.”</span> <em>Remote Sensing</em> 10 (5): 661. <a href="https://doi.org/10.3390/rs10050661">https://doi.org/10.3390/rs10050661</a>.
</div>
<div id="ref-DublinLidar2015" class="csl-entry" role="listitem">
Laefer, Debra F., Saleh Abuwarda, Anh-Vu Vo, Linh Truong-Hong, and Hamid Gharibi. 2017. <span>“2015 Aerial Laser and Photogrammetry Datasets for Dublin, Ireland’s City Center.”</span> New York University. Center for Urban Science; Progress. <a href="https://doi.org/10.17609/N8MQ0N">https://doi.org/10.17609/N8MQ0N</a>.
</div>
<div id="ref-Patent2023" class="csl-entry" role="listitem">
Liu, C.-J., M. Ulicny, and R. Dahyot. 2023. Context aware object geotagging. 18087227, issued 2023. <a href="https://patents.google.com/patent/US20230206402A1/en">https://patents.google.com/patent/US20230206402A1/en</a>.
</div>
<div id="ref-ChaoImvip2021" class="csl-entry" role="listitem">
Liu, C.-J., M. Ulicny, M. Manzke, and R. Dahyot. 2021. <span>“Context Aware Object Geotagging.”</span> In <em>Irish Machine Vision and Image Processing (IMVIP 2021)</em>. <a href="https://doi.org/10.48550/arXiv.2108.06302">https://doi.org/10.48550/arXiv.2108.06302</a>.
</div>
<div id="ref-GeoGraphECCV2020" class="csl-entry" role="listitem">
Nassar, Ahmed Samy, Stefano D’Aronco, Sébastien Lefèvre, and Jan D. Wegner. 2020. <span>“GeoGraph: Graph-Based Multi-View Object Detection with Geometric Cues End-to-End.”</span> In <em>Computer Vision – ECCV 2020</em>, edited by Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, 488–504. Cham: Springer International Publishing.
</div>
<div id="ref-ULICNY2022108707" class="csl-entry" role="listitem">
Ulicny, M., V. A. Krylov, and R. Dahyot. 2022. <span>“Harmonic Convolutional Networks Based on Discrete Cosine Transform.”</span> <em>Pattern Recognition</em> 129: 1–12. <a href="https://doi.org/10.1016/j.patcog.2022.108707">https://doi.org/10.1016/j.patcog.2022.108707</a>.
</div>
</div>

</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="MU.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://roznn.github.io/">Rozenn Dahyot</a> (<a href="https://www.maynoothuniversity.ie/">maynoothuniversity.ie</a>)</p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="IEEEWISP2025_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="IEEEWISP2025_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="IEEEWISP2025_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="IEEEWISP2025_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="IEEEWISP2025_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="IEEEWISP2025_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="IEEEWISP2025_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="IEEEWISP2025_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="IEEEWISP2025_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="IEEEWISP2025_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="IEEEWISP2025_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":false},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: true,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    <script>videojs(video_shortcode_videojs_video1);</script>
    

</body></html>